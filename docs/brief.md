# Project Brief: Система извлечения артикулов автозапчастей

## Executive Summary

**Проект:** Система автоматического извлечения артикулов автозапчастей из текстов объявлений

**Концепция:** Автоматизированный инструмент для извлечения артикулов и брендов автозапчастей из массива объявлений Avito для последующей работы с ними.

**Основная проблема:** Ручное извлечение артикулов из тысяч объявлений крайне трудозатратно и неэффективно.

**Целевой рынок:** Личное использование для работы с артикулами автозапчастей.

**Ключевая ценность:** Автоматизация извлечения артикулов и брендов из неструктурированных текстов с высокой точностью благодаря каскадному алгоритму поиска (сначала бренды → затем их артикулы).

## Problem Statement

Тысячи объявлений на Avito содержат информацию об артикулах автозапчастей в неструктурированном виде. Артикулы могут быть написаны с использованием как кириллицы, так и латиницы, с различными вариациями форматирования (дефисы, пробелы, спецсимволы). Ручной поиск и извлечение этих артикулов требует огромных временных затрат и подвержен человеческим ошибкам. Необходим автоматизированный инструмент для быстрого и точного извлечения артикулов.

## Proposed Solution

Разработка системы на Python с использованием алгоритма Aho-Corasick для эффективного поиска множественных паттернов. Система будет:
- Загружать данные из PostgreSQL БД
- Нормализовать тексты (приведение к единому формату)
- Использовать каскадный поиск: сначала находить бренды, затем искать только артикулы этих брендов
- Кешировать построенные автоматы для производительности
- Сохранять результаты в структурированном виде в БД

## Target Users

**Основной пользователь:** Импортёр автозапчастей
- Нуждается в быстром извлечении артикулов из объявлений
- Работает с большими объёмами данных
- Требует высокой точности для корректной идентификации запчастей

## Goals & Success Metrics

### Business Objectives
- Автоматизировать процесс извлечения артикулов на 100%
- Достичь точности извлечения артикулов >95%
- Обрабатывать 10000+ объявлений за сеанс работы

### User Success Metrics
- Время обработки одного объявления < 100мс
- Минимальное количество ложных срабатываний
- Простота запуска и использования системы

### Key Performance Indicators (KPIs)
- **Точность извлечения:** процент корректно найденных артикулов
- **Скорость обработки:** объявлений в секунду
- **Полнота покрытия:** процент обработанных объявлений без ошибок

## MVP Scope

### Core Features (Must Have)
- **Подключение к БД PostgreSQL:** чтение данных из таблиц special_model_data и text_model_data
- **Нормализация текста:** приведение к UPPERCASE, замена кириллицы на латиницу, очистка спецсимволов
- **Загрузка CSV-словаря:** обработка ~2 млн записей с артикулами и брендами
- **Каскадный поиск:** сначала бренды через Aho-Corasick, затем артикулы найденных брендов
- **Сохранение результатов:** запись в таблицу avito_parts_resolved с полями first_article, brand_near_first_article, all_articles, all_brands
- **Кеширование автоматов:** сохранение в pickle/json для повышения производительности

### Out of Scope for MVP
- Веб-интерфейс
- API для внешних систем
- Обработка изображений из объявлений
- Машинное обучение для улучшения точности
- Интеграция с внешними каталогами запчастей

### MVP Success Criteria
Система должна успешно обрабатывать тестовый набор из 1000 объявлений, корректно извлекая артикулы с точностью >90% и сохраняя результаты в БД.

## Technical Considerations

### Platform Requirements
- **ОС:** Linux/MacOS/Windows с Python 3.11+
- **База данных:** PostgreSQL 14+
- **RAM:** минимум 8GB для работы с автоматами
- **Дисковое пространство:** 10GB для кеша автоматов

### Technology Preferences
- **Язык:** Python 3.11+
- **Основная библиотека:** pyahocorasick (последняя версия)
- **БД:** PostgreSQL с psycopg2/asyncpg
- **Кеширование:** pickle для автоматов, JSON для метаданных
- **Конфигурация:** python-dotenv для .env файлов

### Architecture Considerations
- **Модульная архитектура:** отдельные модули для нормализации, поиска, работы с БД
- **Асинхронная обработка:** для работы с БД и большими объёмами данных
- **Система логирования:** для отладки и мониторинга процесса
- **Конфигурируемые параметры:** MIN_ARTICLE_LEN_DIGITS, MIN_ARTICLE_LEN_ALPHANUM через .env

## Constraints & Assumptions

### Constraints
- **Объём данных:** ~2 млн записей в словаре артикулов
- **Производительность:** обработка должна быть достаточно быстрой для практического использования
- **Точность:** баланс между скоростью и точностью извлечения

### Key Assumptions
- Структура БД Avito остаётся стабильной
- Большинство артикулов в объявлениях соответствуют записям в CSV-словаре
- Нормализация текста покрывает большинство вариаций написания
- Каскадный подход (бренд→артикул) повышает точность

## Risks & Open Questions

### Key Risks
- **Изменение структуры БД:** может потребовать адаптации SQL-запросов
- **Неполнота словаря:** артикулы, отсутствующие в CSV, не будут найдены
- **Производительность:** обработка миллионов записей может быть медленной

### Open Questions
- Как часто обновляется CSV-словарь артикулов?
- Нужна ли инкрементальная обработка новых объявлений?
- Требуется ли версионирование результатов?

### Areas Needing Further Research
- Оптимальные параметры для MIN_ARTICLE_LEN
- Эффективность различных стратегий кеширования
- Возможности параллельной обработки

## Next Steps

### Immediate Actions
1. Настроить окружение разработки с Python 3.11+ и PostgreSQL
2. Создать структуру проекта и базовые модули
3. Реализовать загрузку и парсинг CSV-словаря
4. Разработать модуль нормализации текста
5. Имплементировать каскадный поиск с Aho-Corasick
6. Настроить подключение к БД и создать таблицу результатов
7. Реализовать систему кеширования автоматов
8. Провести тестирование на небольшой выборке
9. Оптимизировать производительность
10. Запустить полную обработку данных